#!/usr/bin/env python3
"""
Python Uber Data Analysis - Comprehensive Statistical Analysis
Professional data quality assessment and outlier detection for Uber ride data
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
from pathlib import Path
from scipy import stats
from scipy.stats import shapiro, kstest, jarque_bera, anderson
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# Grafik ayarlarÄ±
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

def download_dataset():
    """Download Uber dataset from Kaggle"""
    print("Downloading dataset...")
    try:
        path = kagglehub.dataset_download("yashdevladdha/uber-ride-analytics-dashboard")
        print(f"Dataset successfully downloaded: {path}")
        return path
    except Exception as e:
        print(f"Dataset download error: {e}")
        return None

def load_and_explore_data(dataset_path):
    """Load dataset and perform basic exploratory analysis"""
    if not dataset_path:
        print("Invalid dataset path!")
        return None
    
    # List files in dataset directory
    dataset_dir = Path(dataset_path)
    print(f"\nDataset directory: {dataset_dir}")
    print("Files in directory:")
    
    csv_files = []
    for file in dataset_dir.iterdir():
        if file.is_file():
            print(f"  - {file.name} ({file.stat().st_size / 1024:.1f} KB)")
            if file.suffix.lower() == '.csv':
                csv_files.append(file)
    
    if not csv_files:
        print("No CSV file found!")
        return None
    
    # Ä°lk CSV dosyasÄ±nÄ± yÃ¼kle (genellikle ana veri dosyasÄ±)
    main_data_file = csv_files[0]
    print(f"\nAna veri dosyasÄ± yÃ¼kleniyor: {main_data_file.name}")
    
    try:
        df = pd.read_csv(main_data_file)
        print(f"Veri baÅŸarÄ±yla yÃ¼klendi! Boyut: {df.shape}")
        return df, csv_files
    except Exception as e:
        print(f"Veri yÃ¼kleme hatasÄ±: {e}")
        return None, []

def basic_data_analysis(df):
    """Temel veri analizi ve Ã¶zet istatistikler"""
    print("\n" + "="*60)
    print("BASIC DATA ANALYSIS")
    print("="*60)
    
    # Veri seti hakkÄ±nda genel bilgi
    print(f"\nVeri Seti Boyutu: {df.shape[0]:,} satÄ±r, {df.shape[1]} sÃ¼tun")
    print(f"Bellek KullanÄ±mÄ±: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # SÃ¼tun bilgileri
    print("\nColumn Information:")
    print("-" * 40)
    for col in df.columns:
        dtype = df[col].dtype
        null_count = df[col].isnull().sum()
        null_pct = (null_count / len(df)) * 100
        unique_count = df[col].nunique()
        print(f"{col:20} | {str(dtype):12} | Null: {null_count:6} ({null_pct:5.1f}%) | Unique: {unique_count:6}")
    
    # Ä°lk ve son birkaÃ§ satÄ±r
    print("\nFirst 5 Rows:")
    print("-" * 40)
    print(df.head())
    
    print("\nLast 5 Rows:")
    print("-" * 40)
    print(df.tail())
    
    # SayÄ±sal sÃ¼tunlar iÃ§in Ã¶zet istatistikler
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print("\nNumeric Columns - Summary Statistics:")
        print("-" * 40)
        print(df[numeric_cols].describe())
    
    # Kategorik sÃ¼tunlar iÃ§in en sÄ±k deÄŸerler
    categorical_cols = df.select_dtypes(include=['object']).columns
    if len(categorical_cols) > 0:
        print("\nCategorical Columns - Most Frequent Values:")
        print("-" * 40)
        for col in categorical_cols[:5]:  # Ä°lk 5 kategorik sÃ¼tun
            print(f"\n{col}:")
            print(df[col].value_counts().head())
    
    return df

def create_initial_visualizations(df):
    """Ä°lk gÃ¶rselleÅŸtirmeleri oluÅŸtur"""
    print("\n" + "="*60)
    print("BASIC VISUALIZATIONS")
    print("="*60)
    
    # SayÄ±sal sÃ¼tunlar iÃ§in histogramlar
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        n_cols = min(len(numeric_cols), 4)
        n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))
        if n_rows == 1:
            axes = [axes] if n_cols == 1 else axes
        else:
            axes = axes.flatten()
        
        for i, col in enumerate(numeric_cols):
            if i < len(axes):
                df[col].hist(bins=30, ax=axes[i], alpha=0.7)
                axes[i].set_title(f'{col} DaÄŸÄ±lÄ±mÄ±')
                axes[i].set_xlabel(col)
                axes[i].set_ylabel('Frekans')
        
        # KullanÄ±lmayan subplot'larÄ± gizle
        for i in range(len(numeric_cols), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.savefig('/Users/mertcagatay/Desktop/BitirmeDenemeler/numeric_distributions.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("Distribution plots for numeric variables created.")
    
    # Eksik deÄŸer analizi
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        plt.figure(figsize=(12, 6))
        null_counts[null_counts > 0].plot(kind='bar')
        plt.title('SÃ¼tunlara GÃ¶re Eksik DeÄŸer SayÄ±larÄ±')
        plt.xlabel('SÃ¼tunlar')
        plt.ylabel('Eksik DeÄŸer SayÄ±sÄ±')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('/Users/mertcagatay/Desktop/BitirmeDenemeler/missing_values.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("Missing value analysis plot created.")

def statistical_reliability_tests(df):
    """Veri gÃ¼venirliÄŸi iÃ§in kapsamlÄ± istatistiksel testler"""
    print("\n" + "="*80)
    print("STATISTICAL RELIABILITY TESTS")
    print("="*80)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    results = {}
    
    if len(numeric_cols) == 0:
        print("âŒ No numeric columns found!")
        return results
    
    # 1. NORMALLÄ°K TESTLERÄ°
    print("\nğŸ” 1. NORMALITY TESTS")
    print("-" * 50)
    
    normality_results = {}
    for col in numeric_cols:
        data = df[col].dropna()
        if len(data) < 3:
            continue
            
        print(f"\nğŸ“Š {col} sÃ¼tunu:")
        
        # Shapiro-Wilk Test (n < 5000 iÃ§in ideal)
        if len(data) <= 5000:
            shapiro_stat, shapiro_p = shapiro(data.sample(min(5000, len(data))))
            print(f"   Shapiro-Wilk: stat={shapiro_stat:.4f}, p-value={shapiro_p:.4f}")
            is_normal_shapiro = shapiro_p > 0.05
        else:
            is_normal_shapiro = None
            print(f"   Shapiro-Wilk: Veri Ã§ok bÃ¼yÃ¼k (n={len(data)})")
        
        # Kolmogorov-Smirnov Test
        ks_stat, ks_p = kstest(data, 'norm', args=(data.mean(), data.std()))
        print(f"   Kolmogorov-Smirnov: stat={ks_stat:.4f}, p-value={ks_p:.4f}")
        is_normal_ks = ks_p > 0.05
        
        # Jarque-Bera Test
        jb_stat, jb_p = jarque_bera(data)
        print(f"   Jarque-Bera: stat={jb_stat:.4f}, p-value={jb_p:.4f}")
        is_normal_jb = jb_p > 0.05
        
        # Anderson-Darling Test
        ad_result = anderson(data, dist='norm')
        is_normal_ad = ad_result.statistic < ad_result.critical_values[2]  # %5 seviyesi
        print(f"   Anderson-Darling: stat={ad_result.statistic:.4f}")
        
        # Ã–zet
        normal_tests = [is_normal_ks, is_normal_jb, is_normal_ad]
        if is_normal_shapiro is not None:
            normal_tests.append(is_normal_shapiro)
        
        normal_ratio = sum(normal_tests) / len(normal_tests)
        
        if normal_ratio >= 0.5:
            print(f"   âœ… SonuÃ§: Normal daÄŸÄ±lÄ±m ({normal_ratio:.1%} testler normal)")
        else:
            print(f"   âŒ SonuÃ§: Normal deÄŸil ({normal_ratio:.1%} testler normal)")
        
        normality_results[col] = {
            'shapiro_p': shapiro_p if is_normal_shapiro is not None else None,
            'ks_p': ks_p,
            'jb_p': jb_p,
            'anderson_stat': ad_result.statistic,
            'is_normal': normal_ratio >= 0.5
        }
    
    # 2. OUTLIER TESPÄ°TÄ°
    print("\nğŸ¯ 2. OUTLIER DETECTION")
    print("-" * 50)
    
    outlier_results = {}
    for col in numeric_cols:
        data = df[col].dropna()
        if len(data) < 3:
            continue
            
        print(f"\nğŸ“Š {col} sÃ¼tunu:")
        
        # IQR YÃ¶ntemi
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        iqr_outliers = ((data < lower_bound) | (data > upper_bound)).sum()
        iqr_percentage = (iqr_outliers / len(data)) * 100
        
        # Z-Score YÃ¶ntemi
        z_scores = np.abs(stats.zscore(data))
        z_outliers = (z_scores > 3).sum()
        z_percentage = (z_outliers / len(data)) * 100
        
        # Modified Z-Score (Robust)
        median = np.median(data)
        mad = np.median(np.abs(data - median))
        modified_z_scores = 0.6745 * (data - median) / mad
        modified_z_outliers = (np.abs(modified_z_scores) > 3.5).sum()
        modified_z_percentage = (modified_z_outliers / len(data)) * 100
        
        print(f"   IQR YÃ¶ntemi: {iqr_outliers} aykÄ±rÄ± deÄŸer ({iqr_percentage:.2f}%)")
        print(f"   Z-Score: {z_outliers} aykÄ±rÄ± deÄŸer ({z_percentage:.2f}%)")
        print(f"   Modified Z-Score: {modified_z_outliers} aykÄ±rÄ± deÄŸer ({modified_z_percentage:.2f}%)")
        
        # Outlier deÄŸerlendirmesi
        avg_outlier_pct = (iqr_percentage + z_percentage + modified_z_percentage) / 3
        if avg_outlier_pct < 5:
            print(f"   âœ… Kabul edilebilir seviye ({avg_outlier_pct:.1f}% ortalama)")
        elif avg_outlier_pct < 10:
            print(f"   âš ï¸  Dikkat gerekli ({avg_outlier_pct:.1f}% ortalama)")
        else:
            print(f"   âŒ YÃ¼ksek aykÄ±rÄ± deÄŸer oranÄ± ({avg_outlier_pct:.1f}% ortalama)")
        
        outlier_results[col] = {
            'iqr_count': iqr_outliers,
            'iqr_percentage': iqr_percentage,
            'z_count': z_outliers,
            'z_percentage': z_percentage,
            'modified_z_count': modified_z_outliers,
            'modified_z_percentage': modified_z_percentage,
            'average_percentage': avg_outlier_pct
        }
    
    # 3. KORELASYON VE Ã‡OKLU BAÄINTI ANALÄ°ZÄ°
    print("\nğŸ”— 3. KORELASYON VE Ã‡OKLU BAÄINTI ANALÄ°ZÄ°")
    print("-" * 50)
    
    correlation_matrix = df[numeric_cols].corr()
    
    # YÃ¼ksek korelasyonlarÄ± bul
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = correlation_matrix.iloc[i, j]
            if abs(corr_val) > 0.8:
                high_corr_pairs.append((
                    correlation_matrix.columns[i],
                    correlation_matrix.columns[j],
                    corr_val
                ))
    
    print(f"Toplam {len(numeric_cols)} sayÄ±sal deÄŸiÅŸken arasÄ±nda:")
    if high_corr_pairs:
        print("âš ï¸  YÃ¼ksek korelasyon tespit edildi (|r| > 0.8):")
        for var1, var2, corr in high_corr_pairs:
            print(f"   {var1} â†” {var2}: r = {corr:.3f}")
    else:
        print("âœ… YÃ¼ksek korelasyon (|r| > 0.8) tespit edilmedi")
    
    # VIF (Variance Inflation Factor) hesaplama
    try:
        from statsmodels.stats.outliers_influence import variance_inflation_factor
        
        # Eksik deÄŸerleri Ã§Ä±kar ve geÃ§erli sÃ¼tunlarÄ± filtrele
        clean_df = df[numeric_cols].dropna()
        
        if len(clean_df) == 0:
            print("âš ï¸  VIF hesaplamasÄ± iÃ§in yeterli temiz veri yok")
        elif len(clean_df.columns) < 2:
            print("âš ï¸  VIF hesaplamasÄ± iÃ§in en az 2 deÄŸiÅŸken gerekli")
        else:
            # Sabit olmayan sÃ¼tunlarÄ± bul
            non_constant_cols = []
            for col in clean_df.columns:
                if clean_df[col].nunique() > 1:  # Sabit olmayan sÃ¼tunlar
                    non_constant_cols.append(col)
            
            if len(non_constant_cols) < 2:
                print("âš ï¸  VIF hesaplamasÄ± iÃ§in en az 2 deÄŸiÅŸken sÃ¼tun gerekli")
            else:
                # Standardize et
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(clean_df[non_constant_cols])
                
                vif_data = pd.DataFrame()
                vif_data["DeÄŸiÅŸken"] = non_constant_cols
                vif_data["VIF"] = [variance_inflation_factor(scaled_data, i) 
                                  for i in range(len(non_constant_cols))]
                
                print(f"\nVIF (Variance Inflation Factor) DeÄŸerleri:")
                print(vif_data.to_string(index=False))
                
                high_vif = vif_data[vif_data["VIF"] > 10]
                if len(high_vif) > 0:
                    print("âš ï¸  YÃ¼ksek VIF deÄŸerleri (>10) - Ã‡oklu baÄŸÄ±ntÄ± riski:")
                    print(high_vif.to_string(index=False))
                else:
                    print("âœ… TÃ¼m VIF deÄŸerleri kabul edilebilir seviyede (<10)")
            
    except ImportError:
        print("âš ï¸  VIF hesaplamasÄ± iÃ§in statsmodels gerekli (pip install statsmodels)")
    except Exception as e:
        print(f"âš ï¸  VIF hesaplama hatasÄ±: {e}")
    
    # 4. VERÄ° TUTARLILIÄI TESTLERÄ°
    print("\nğŸ“‹ 4. DATA CONSISTENCY TESTS")
    print("-" * 50)
    
    consistency_issues = []
    
    # Negatif deÄŸerler kontrolÃ¼ (olmamasÄ± gereken yerlerde)
    negative_checks = ['fare_amount', 'distance', 'duration', 'tip_amount']
    for col in negative_checks:
        if col in df.columns:
            negative_count = (df[col] < 0).sum()
            if negative_count > 0:
                consistency_issues.append(f"{col}: {negative_count} negatif deÄŸer")
    
    # SÄ±fÄ±r deÄŸerler kontrolÃ¼
    zero_checks = ['fare_amount', 'distance']
    for col in zero_checks:
        if col in df.columns:
            zero_count = (df[col] == 0).sum()
            zero_percentage = (zero_count / len(df)) * 100
            if zero_percentage > 5:
                consistency_issues.append(f"{col}: {zero_count} sÄ±fÄ±r deÄŸer ({zero_percentage:.1f}%)")
    
    # AÅŸÄ±rÄ± yÃ¼ksek deÄŸerler
    for col in numeric_cols:
        data = df[col].dropna()
        if len(data) > 0:
            upper_99 = data.quantile(0.99)
            upper_999 = data.quantile(0.999)
            extreme_count = (data > upper_999 * 10).sum()  # %99.9'dan 10 kat bÃ¼yÃ¼k
            if extreme_count > 0:
                consistency_issues.append(f"{col}: {extreme_count} aÅŸÄ±rÄ± yÃ¼ksek deÄŸer")
    
    if consistency_issues:
        print("âš ï¸  TutarlÄ±lÄ±k sorunlarÄ± tespit edildi:")
        for issue in consistency_issues:
            print(f"   - {issue}")
    else:
        print("âœ… Veri tutarlÄ±lÄ±ÄŸÄ± kontrollerinde sorun bulunamadÄ±")
    
    # SonuÃ§larÄ± toplat
    results = {
        'normality': normality_results,
        'outliers': outlier_results,
        'high_correlations': high_corr_pairs,
        'consistency_issues': consistency_issues
    }
    
    return results

def clean_outliers(df, method='iqr', threshold=1.5):
    """Outlier'larÄ± temizle ve temizlenmiÅŸ veri setini dÃ¶ndÃ¼r"""
    print(f"\nğŸ§¹ OUTLIER TEMÄ°ZLEME - {method.upper()} YÃ¶ntemi")
    print("-" * 50)
    
    df_clean = df.copy()
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    removed_count = 0
    
    outlier_summary = {}
    
    for col in numeric_cols:
        if col not in df_clean.columns:
            continue
            
        original_count = len(df_clean)
        data = df_clean[col].dropna()
        
        if len(data) < 3:
            continue
            
        if method == 'iqr':
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            
            # Outlier'larÄ± tespit et - sadece null olmayan deÄŸerlerde
            outlier_mask = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)
            # NaN deÄŸerleri False olarak ayarla
            outlier_mask = outlier_mask.fillna(False)
            
        elif method == 'zscore':
            # Z-score iÃ§in sadece null olmayan deÄŸerleri kullan
            valid_indices = df_clean[col].notna()
            if valid_indices.sum() < 3:
                continue
                
            z_scores = pd.Series(index=df_clean.index, dtype=float)
            z_scores[valid_indices] = np.abs(stats.zscore(df_clean.loc[valid_indices, col]))
            outlier_mask = z_scores > threshold
            outlier_mask = outlier_mask.fillna(False)
            
        elif method == 'modified_zscore':
            # Modified Z-score iÃ§in sadece null olmayan deÄŸerleri kullan
            valid_indices = df_clean[col].notna()
            if valid_indices.sum() < 3:
                continue
                
            valid_data = df_clean.loc[valid_indices, col]
            median = np.median(valid_data)
            mad = np.median(np.abs(valid_data - median))
            
            if mad == 0:  # MAD sÄ±fÄ±r ise skip et
                continue
                
            modified_z_scores = pd.Series(index=df_clean.index, dtype=float)
            modified_z_scores[valid_indices] = 0.6745 * (valid_data - median) / mad
            outlier_mask = np.abs(modified_z_scores) > threshold
            outlier_mask = outlier_mask.fillna(False)
        
        # Outlier'larÄ± Ã§Ä±kar
        outliers_before = outlier_mask.sum()
        df_clean = df_clean[~outlier_mask]
        outliers_removed = original_count - len(df_clean)
        removed_count += outliers_removed
        
        outlier_summary[col] = {
            'outliers_detected': outliers_before,
            'outliers_removed': outliers_removed,
            'percentage_removed': (outliers_removed / original_count) * 100
        }
        
        if outliers_removed > 0:
            print(f"   {col}: {outliers_removed} outlier Ã§Ä±karÄ±ldÄ± ({(outliers_removed/original_count)*100:.2f}%)")
    
    print(f"\nâœ… Toplam {removed_count} outlier Ã§Ä±karÄ±ldÄ±")
    print(f"ğŸ“Š Orijinal boyut: {len(df):,} â†’ TemizlenmiÅŸ boyut: {len(df_clean):,}")
    print(f"ğŸ¯ Veri kaybÄ±: {((len(df) - len(df_clean)) / len(df)) * 100:.2f}%")
    
    return df_clean, outlier_summary

def enhanced_correlation_analysis(df):
    """GeliÅŸmiÅŸ korelasyon analizi - dÃ¼ÅŸÃ¼k korelasyonlarÄ± da dahil et"""
    print(f"\nğŸ” GELÄ°ÅMÄ°Å KORELASYON ANALÄ°ZÄ°")
    print("-" * 50)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) < 2:
        print("âŒ En az 2 sayÄ±sal deÄŸiÅŸken gerekli!")
        return {}
    
    correlation_matrix = df[numeric_cols].corr()
    
    # Korelasyon kategorileri
    correlation_categories = {
        'very_high': [],      # |r| > 0.8
        'high': [],           # 0.6 < |r| <= 0.8
        'moderate': [],       # 0.3 < |r| <= 0.6
        'low': [],            # 0.1 < |r| <= 0.3
        'very_low': []        # |r| <= 0.1
    }
    
    # TÃ¼m korelasyon Ã§iftlerini kategorize et
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            var1 = correlation_matrix.columns[i]
            var2 = correlation_matrix.columns[j]
            corr_val = correlation_matrix.iloc[i, j]
            abs_corr = abs(corr_val)
            
            pair_info = (var1, var2, corr_val)
            
            if abs_corr > 0.8:
                correlation_categories['very_high'].append(pair_info)
            elif abs_corr > 0.6:
                correlation_categories['high'].append(pair_info)
            elif abs_corr > 0.3:
                correlation_categories['moderate'].append(pair_info)
            elif abs_corr > 0.1:
                correlation_categories['low'].append(pair_info)
            else:
                correlation_categories['very_low'].append(pair_info)
    
    # SonuÃ§larÄ± gÃ¶ster
    print(f"ğŸ“Š Toplam {len(numeric_cols)} deÄŸiÅŸken arasÄ±nda {len(numeric_cols)*(len(numeric_cols)-1)//2} Ã§ift analiz edildi:\n")
    
    if correlation_categories['very_high']:
        print("ğŸ”´ Ã‡OK YÃœKSEK Korelasyonlar (|r| > 0.8) - Ã‡oklu baÄŸÄ±ntÄ± riski:")
        for var1, var2, corr in correlation_categories['very_high']:
            print(f"   {var1} â†” {var2}: r = {corr:.3f}")
    
    if correlation_categories['high']:
        print("\nğŸŸ  YÃœKSEK Korelasyonlar (0.6 < |r| â‰¤ 0.8):")
        for var1, var2, corr in correlation_categories['high']:
            print(f"   {var1} â†” {var2}: r = {corr:.3f}")
    
    if correlation_categories['moderate']:
        print("\nğŸŸ¡ ORTA Korelasyonlar (0.3 < |r| â‰¤ 0.6):")
        for var1, var2, corr in correlation_categories['moderate']:
            print(f"   {var1} â†” {var2}: r = {corr:.3f}")
    
    if correlation_categories['low']:
        print("\nğŸŸ¢ DÃœÅÃœK Korelasyonlar (0.1 < |r| â‰¤ 0.3) - Dikkat edilmesi gerekenler:")
        for var1, var2, corr in correlation_categories['low']:
            print(f"   {var1} â†” {var2}: r = {corr:.3f}")
    
    # Uber verisi iÃ§in domain spesifik mantÄ±k kontrolleri
    print(f"\nğŸš— UBER VERÄ°SÄ° Ã–ZEL KONTROLLER:")
    print("-" * 30)
    
    uber_logical_pairs = [
        ('distance', 'duration', 'Mesafe-SÃ¼re pozitif korelasyon beklenir'),
        ('distance', 'fare', 'Mesafe-Ãœcret pozitif korelasyon beklenir'),
        ('duration', 'fare', 'SÃ¼re-Ãœcret pozitif korelasyon beklenir'),
        ('distance', 'tip', 'Mesafe-BahÅŸiÅŸ zayÄ±f pozitif olabilir'),
        ('fare', 'tip', 'Ãœcret-BahÅŸiÅŸ pozitif korelasyon beklenir'),
        ('rating', 'tip', 'DeÄŸerlendirme-BahÅŸiÅŸ pozitif korelasyon beklenir')
    ]
    
    domain_issues = []
    for var1_pattern, var2_pattern, explanation in uber_logical_pairs:
        # SÃ¼tun isimlerinde pattern ara
        var1_matches = [col for col in numeric_cols if var1_pattern.lower() in col.lower()]
        var2_matches = [col for col in numeric_cols if var2_pattern.lower() in col.lower()]
        
        for v1 in var1_matches:
            for v2 in var2_matches:
                if v1 != v2 and v1 in correlation_matrix.columns and v2 in correlation_matrix.columns:
                    corr_val = correlation_matrix.loc[v1, v2]
                    
                    # MantÄ±k kontrolleri
                    if 'pozitif' in explanation and corr_val < 0:
                        domain_issues.append(f"âš ï¸  {v1} â†” {v2}: r={corr_val:.3f} (Negatif, ama {explanation})")
                    elif 'pozitif' in explanation and abs(corr_val) < 0.1:
                        domain_issues.append(f"ğŸ’¡ {v1} â†” {v2}: r={corr_val:.3f} (Ã‡ok zayÄ±f, {explanation})")
                    elif abs(corr_val) > 0.3:
                        print(f"âœ… {v1} â†” {v2}: r={corr_val:.3f} (Beklenen - {explanation})")
    
    if domain_issues:
        print("\nğŸ” MantÄ±k dÄ±ÅŸÄ± korelasyonlar:")
        for issue in domain_issues:
            print(f"   {issue}")
    else:
        print("âœ… Domain mantÄ±ÄŸÄ± aÃ§Ä±sÄ±ndan anormal korelasyon tespit edilmedi")
    
    return correlation_categories

def compare_before_after_cleaning(df_original, df_clean, outlier_summary):
    """Temizleme Ã¶ncesi ve sonrasÄ± karÅŸÄ±laÅŸtÄ±rma"""
    print(f"\nğŸ“Š TEMÄ°ZLEME Ã–NCESÄ° VS SONRASI KARÅILAÅTIRMA")
    print("="*60)
    
    numeric_cols = df_original.select_dtypes(include=[np.number]).columns
    
    comparison_results = {}
    
    print(f"ğŸ“ˆ Ä°statistiksel DeÄŸiÅŸimler:")
    print("-" * 40)
    
    for col in numeric_cols:
        if col in df_clean.columns:
            orig_data = df_original[col].dropna()
            clean_data = df_clean[col].dropna()
            
            if len(orig_data) > 0 and len(clean_data) > 0:
                # Ä°statistikler
                orig_mean = orig_data.mean()
                clean_mean = clean_data.mean()
                orig_std = orig_data.std()
                clean_std = clean_data.std()
                orig_median = orig_data.median()
                clean_median = clean_data.median()
                
                # DeÄŸiÅŸim oranlarÄ± - sÄ±fÄ±ra bÃ¶lme kontrolÃ¼
                mean_change = ((clean_mean - orig_mean) / orig_mean) * 100 if orig_mean != 0 else 0
                std_change = ((clean_std - orig_std) / orig_std) * 100 if orig_std != 0 else 0
                median_change = ((clean_median - orig_median) / orig_median) * 100 if orig_median != 0 else 0
                
                print(f"\nğŸ“Š {col}:")
                print(f"   Ortalama: {orig_mean:.2f} â†’ {clean_mean:.2f} ({mean_change:+.1f}%)")
                print(f"   Std Sapma: {orig_std:.2f} â†’ {clean_std:.2f} ({std_change:+.1f}%)")
                print(f"   Medyan: {orig_median:.2f} â†’ {clean_median:.2f} ({median_change:+.1f}%)")
                
                comparison_results[col] = {
                    'mean_change_pct': mean_change,
                    'std_change_pct': std_change,
                    'median_change_pct': median_change,
                    'outliers_removed': outlier_summary.get(col, {}).get('outliers_removed', 0)
                }
    
    # Genel deÄŸerlendirme
    total_outliers = sum(info.get('outliers_removed', 0) for info in outlier_summary.values())
    data_loss_pct = ((len(df_original) - len(df_clean)) / len(df_original)) * 100
    
    print(f"\nğŸ¯ GENEL DEÄERLENDÄ°RME:")
    print(f"   Toplam outlier Ã§Ä±karÄ±ldÄ±: {total_outliers:,}")
    print(f"   Veri kaybÄ±: {data_loss_pct:.2f}%")
    
    if data_loss_pct < 5:
        print("   âœ… Kabul edilebilir veri kaybÄ±")
    elif data_loss_pct < 10:
        print("   âš ï¸  Orta seviye veri kaybÄ± - dikkatli ol")
    else:
        print("   âŒ YÃ¼ksek veri kaybÄ± - stratejini gÃ¶zden geÃ§ir")
    
    return comparison_results

def create_cleaning_workflow(df, methods=['iqr'], thresholds=[1.5]):
    """Outlier temizleme iÅŸ akÄ±ÅŸÄ± - farklÄ± yÃ¶ntemleri dene"""
    print(f"\nğŸ”„ OUTLIER TEMÄ°ZLEME Ä°Å AKIÅI")
    print("="*60)
    
    results = {}
    
    for method in methods:
        for threshold in thresholds:
            print(f"\nğŸ“‹ {method.upper()} yÃ¶ntemi, eÅŸik: {threshold}")
            print("-" * 40)
            
            df_cleaned, outlier_summary = clean_outliers(df, method=method, threshold=threshold)
            
            # Bu temizleme iÃ§in gÃ¼venirlik testini tekrarla
            reliability_results = statistical_reliability_tests(df_cleaned)
            
            # KarÅŸÄ±laÅŸtÄ±rma
            comparison = compare_before_after_cleaning(df, df_cleaned, outlier_summary)
            
            results[f"{method}_{threshold}"] = {
                'cleaned_df': df_cleaned,
                'outlier_summary': outlier_summary,
                'reliability_results': reliability_results,
                'comparison': comparison,
                'data_loss_pct': ((len(df) - len(df_cleaned)) / len(df)) * 100
            }
    
    # En iyi yÃ¶ntemi Ã¶ner
    print(f"\nğŸ† YÃ–NTEM Ã–NERÄ°SÄ°:")
    print("-" * 30)
    
    best_method = None
    best_score = -1
    
    for method_name, result in results.items():
        # Skor hesapla: dÃ¼ÅŸÃ¼k veri kaybÄ± + yÃ¼ksek gÃ¼venirlik
        data_loss = result['data_loss_pct']
        
        # Basit skorlama
        if data_loss < 5:
            loss_score = 10
        elif data_loss < 10:
            loss_score = 7
        elif data_loss < 15:
            loss_score = 4
        else:
            loss_score = 1
            
        total_score = loss_score
        
        if total_score > best_score:
            best_score = total_score
            best_method = method_name
        
        print(f"   {method_name}: Veri kaybÄ± {data_loss:.1f}%, Skor: {total_score}")
    
    print(f"\nğŸ’¡ Ã–NERÄ°LEN: {best_method} yÃ¶ntemi")
    
    return results, best_method

def create_reliability_visualizations(df, results):
    """GÃ¼venirlik analizi gÃ¶rselleÅŸtirmeleri"""
    print("\nğŸ“Š GÃœVENÄ°RLÄ°K ANALÄ°ZÄ° GÃ–RSELLEÅTÄ°RMELERÄ°")
    print("-" * 50)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    # 1. Normallik test sonuÃ§larÄ±
    if 'normality' in results:
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Normallik Test SonuÃ§larÄ±', fontsize=16)
        
        for i, col in enumerate(numeric_cols[:4]):
            if col in results['normality']:
                row, col_idx = i // 2, i % 2
                ax = axes[row, col_idx]
                
                data = df[col].dropna()
                
                # Q-Q Plot
                stats.probplot(data, dist="norm", plot=ax)
                ax.set_title(f'{col} - Q-Q Plot')
                ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/Users/mertcagatay/Desktop/BitirmeDenemeler/normality_tests.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    # 2. Korelasyon Ä±sÄ± haritasÄ±
    if len(numeric_cols) > 1:
        plt.figure(figsize=(12, 10))
        correlation_matrix = df[numeric_cols].corr()
        
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                   center=0, square=True, linewidths=0.5)
        plt.title('DeÄŸiÅŸkenler ArasÄ± Korelasyon Matrisi')
        plt.tight_layout()
        plt.savefig('/Users/mertcagatay/Desktop/BitirmeDenemeler/correlation_matrix.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    # 3. Outlier gÃ¶rselleÅŸtirmesi
    if len(numeric_cols) > 0:
        n_cols = min(len(numeric_cols), 4)
        fig, axes = plt.subplots(1, n_cols, figsize=(4*n_cols, 6))
        if n_cols == 1:
            axes = [axes]
        
        for i, col in enumerate(numeric_cols[:n_cols]):
            ax = axes[i]
            df.boxplot(column=col, ax=ax)
            ax.set_title(f'{col}\nOutlier Analizi')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/Users/mertcagatay/Desktop/BitirmeDenemeler/outlier_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    print("âœ… GÃ¼venirlik analizi gÃ¶rselleÅŸtirmeleri oluÅŸturuldu")

def generate_reliability_report(df, results):
    """GÃ¼venirlik analizi raporu oluÅŸtur"""
    print("\n" + "="*80)
    print("GÃœVENÄ°RLÄ°K ANALÄ°ZÄ° RAPORU")
    print("="*80)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    print(f"\nğŸ“Š VERÄ° SETÄ° Ã–ZETÄ°:")
    print(f"   - Toplam gÃ¶zlem: {len(df):,}")
    print(f"   - SayÄ±sal deÄŸiÅŸken: {len(numeric_cols)}")
    print(f"   - Eksik deÄŸer oranÄ±: {(df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.2f}%")
    
    reliability_score = 0
    max_score = 0
    
    # Normallik skoru
    if 'normality' in results and results['normality']:
        normal_count = sum(1 for r in results['normality'].values() if r['is_normal'])
        normality_score = (normal_count / len(results['normality'])) * 25
        reliability_score += normality_score
        print(f"\nâœ… NORMALLÄ°K SKORU: {normality_score:.1f}/25")
        print(f"   {normal_count}/{len(results['normality'])} deÄŸiÅŸken normal daÄŸÄ±lÄ±mlÄ±")
    max_score += 25
    
    # Outlier skoru
    if 'outliers' in results and results['outliers']:
        good_outlier_count = sum(1 for r in results['outliers'].values() 
                               if r['average_percentage'] < 5)
        outlier_score = (good_outlier_count / len(results['outliers'])) * 25
        reliability_score += outlier_score
        print(f"\nâœ… OUTLIER SKORU: {outlier_score:.1f}/25")
        print(f"   {good_outlier_count}/{len(results['outliers'])} deÄŸiÅŸken kabul edilebilir outlier seviyesinde")
    max_score += 25
    
    # Korelasyon skoru
    correlation_score = 25
    if 'high_correlations' in results:
        high_corr_count = len(results['high_correlations'])
        if high_corr_count > 0:
            correlation_score = max(0, 25 - (high_corr_count * 5))
        print(f"\nâœ… KORELASYON SKORU: {correlation_score:.1f}/25")
        print(f"   {high_corr_count} yÃ¼ksek korelasyon Ã§ifti tespit edildi")
    reliability_score += correlation_score
    max_score += 25
    
    # TutarlÄ±lÄ±k skoru
    consistency_score = 25
    if 'consistency_issues' in results:
        issue_count = len(results['consistency_issues'])
        if issue_count > 0:
            consistency_score = max(0, 25 - (issue_count * 3))
        print(f"\nâœ… TUTARLILIK SKORU: {consistency_score:.1f}/25")
        print(f"   {issue_count} tutarlÄ±lÄ±k sorunu tespit edildi")
    reliability_score += consistency_score
    max_score += 25
    
    # Genel deÄŸerlendirme
    final_score = (reliability_score / max_score) * 100
    
    print(f"\n" + "="*50)
    print(f"ğŸ¯ GENEL GÃœVENÄ°RLÄ°K SKORU: {final_score:.1f}/100")
    
    if final_score >= 85:
        print("ğŸŒŸ MÃœKEMMEL - Veri seti yÃ¼ksek gÃ¼venirlikte")
        recommendation = "Veri seti doÄŸrudan analiz iÃ§in uygun"
    elif final_score >= 70:
        print("âœ… Ä°YÄ° - Veri seti gÃ¼venilir seviyede") 
        recommendation = "Minimal temizleme ile kullanÄ±labilir"
    elif final_score >= 50:
        print("âš ï¸  ORTA - Dikkatli kullanÄ±m gerekli")
        recommendation = "KapsamlÄ± veri temizleme Ã¶nerilir"
    else:
        print("âŒ DÃœÅÃœK - Ciddi veri kalitesi sorunlarÄ±")
        recommendation = "YoÄŸun veri Ã¶n iÅŸleme gerekli"
    
    print(f"ğŸ’¡ Ã–NERÄ°: {recommendation}")
    
    return {
        'final_score': final_score,
        'recommendation': recommendation,
        'detailed_scores': {
            'normality': normality_score if 'normality' in results else 0,
            'outliers': outlier_score if 'outliers' in results else 0,
            'correlation': correlation_score,
            'consistency': consistency_score
        }
    }

def main():
    """Main analysis function"""
    print("Python Uber Data Analysis - Starting Analysis...")
    print("="*60)
    
    # Veri setini indir
    dataset_path = download_dataset()
    
    if dataset_path:
        # Veri setini yÃ¼kle ve keÅŸfet
        result = load_and_explore_data(dataset_path)
        
        if result and result[0] is not None:
            df, csv_files = result
            
            print(f"\n{'='*80}")
            print("ğŸ“Š AÅAMA 1: TAM VERÄ° SETÄ° Ä°LE KAPSAMLI ANALÄ°Z")
            print("="*80)
            
            # 1.1 Temel veri analizi
            print(f"\nğŸ” 1.1 - Temel Veri Analizi")
            print("-" * 50)
            df = basic_data_analysis(df)
            
            # 1.2 Ä°lk gÃ¶rselleÅŸtirmeler
            print(f"\nğŸ“Š 1.2 - Temel GÃ¶rselleÅŸtirmeler")
            print("-" * 50)
            create_initial_visualizations(df)
            
            # 1.3 Ä°statistiksel gÃ¼venirlik testleri (TAM VERÄ°)
            print(f"\nğŸ”¬ 1.3 - Ä°statistiksel GÃ¼venirlik Testleri (TAM VERÄ°)")
            print("-" * 50)
            reliability_results_full = statistical_reliability_tests(df)
            
            # 1.4 GÃ¼venirlik gÃ¶rselleÅŸtirmeleri (TAM VERÄ°)
            print(f"\nğŸ“ˆ 1.4 - GÃ¼venirlik GÃ¶rselleÅŸtirmeleri (TAM VERÄ°)")
            print("-" * 50)
            create_reliability_visualizations(df, reliability_results_full)
            
            # 1.5 GÃ¼venirlik raporu (TAM VERÄ°)
            print(f"\nğŸ“‹ 1.5 - GÃ¼venirlik Raporu (TAM VERÄ°)")
            print("-" * 50)
            final_report_full = generate_reliability_report(df, reliability_results_full)
            
            # 1.6 GeliÅŸmiÅŸ korelasyon analizi (TAM VERÄ°)
            print(f"\nğŸ”— 1.6 - GeliÅŸmiÅŸ Korelasyon Analizi (TAM VERÄ°)")
            print("-" * 50)
            correlation_categories_full = enhanced_correlation_analysis(df)
            
            print(f"\nâœ… AÅAMA 1 TAMAMLANDI - TAM VERÄ° ANALÄ°ZÄ°")
            print(f"ğŸ“Š GÃ¼venirlik Skoru: {final_report_full['final_score']:.1f}/100")
            
            # =====================================================
            
            print(f"\n{'='*80}")
            print("ğŸ§¹ AÅAMA 2: OUTLIER TEMÄ°ZLEME VE YENÄ°DEN ANALÄ°Z")
            print("="*80)
            
            # 2.1 Outlier temizleme iÅŸ akÄ±ÅŸÄ±
            print(f"\nğŸ”„ 2.1 - Outlier Temizleme Ä°ÅŸ AkÄ±ÅŸÄ±")
            print("-" * 50)
            cleaning_results, best_method = create_cleaning_workflow(
                df, 
                methods=['iqr', 'zscore', 'modified_zscore'], 
                thresholds=[1.5, 2.0, 2.5]
            )
            
            # En iyi temizlenmiÅŸ veri setini seÃ§
            best_cleaned_df = cleaning_results[best_method]['cleaned_df']
            
            print(f"\nğŸ† En Ä°yi Temizleme YÃ¶ntemi: {best_method}")
            print(f"ğŸ“‰ Veri KaybÄ±: {cleaning_results[best_method]['data_loss_pct']:.2f}%")
            
            # 2.2 TemizlenmiÅŸ veri ile gÃ¼venirlik testleri
            print(f"\nğŸ”¬ 2.2 - Ä°statistiksel GÃ¼venirlik Testleri (TEMÄ°ZLENMÄ°Å VERÄ°)")
            print("-" * 50)
            reliability_results_clean = statistical_reliability_tests(best_cleaned_df)
            
            # 2.3 TemizlenmiÅŸ veri ile gÃ¼venirlik raporu
            print(f"\nğŸ“‹ 2.3 - GÃ¼venirlik Raporu (TEMÄ°ZLENMÄ°Å VERÄ°)")
            print("-" * 50)
            final_report_clean = generate_reliability_report(best_cleaned_df, reliability_results_clean)
            
            # 2.4 TemizlenmiÅŸ veri ile korelasyon analizi
            print(f"\nğŸ”— 2.4 - GeliÅŸmiÅŸ Korelasyon Analizi (TEMÄ°ZLENMÄ°Å VERÄ°)")
            print("-" * 50)
            correlation_categories_clean = enhanced_correlation_analysis(best_cleaned_df)
            
            # 2.5 KarÅŸÄ±laÅŸtÄ±rmalÄ± sonuÃ§lar
            print(f"\nâš–ï¸  2.5 - Ã–ncesi vs SonrasÄ± KarÅŸÄ±laÅŸtÄ±rma")
            print("-" * 50)
            comparison_results = compare_before_after_cleaning(df, best_cleaned_df, cleaning_results[best_method]['outlier_summary'])
            
            print(f"\n{'='*80}")
            print("ğŸ“ˆ AÅAMA 3: KAPSAMLI KARÅILAÅTIRMA VE SONUÃ‡ RAPORU")
            print("="*80)
            
            # 3.1 GÃ¼venirlik skoru karÅŸÄ±laÅŸtÄ±rmasÄ±
            print(f"\nğŸ“Š 3.1 - GÃ¼venirlik Skoru KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            print("-" * 50)
            print(f"TAM VERÄ° GÃ¼venirlik Skoru: {final_report_full['final_score']:.1f}/100")
            print(f"TEMÄ°ZLENMÄ°Å VERÄ° GÃ¼venirlik Skoru: {final_report_clean['final_score']:.1f}/100")
            improvement = final_report_clean['final_score'] - final_report_full['final_score']
            if improvement > 0:
                print(f"ğŸ¯ Ä°YÄ°LEÅME: +{improvement:.1f} puan (âœ… Outlier temizleme etkili)")
            elif improvement < 0:
                print(f"âš ï¸  DÃœÅÃœÅ: {improvement:.1f} puan (Dikkatli kullanÄ±m gerekli)")
            else:
                print(f"â¡ï¸  DEÄÄ°ÅÄ°M YOK: Outlier temizleme etkisiz")
            
            # 3.2 Korelasyon deÄŸiÅŸimleri
            print(f"\nğŸ”— 3.2 - Korelasyon DeÄŸiÅŸimleri")
            print("-" * 50)
            
            def count_correlations(categories):
                return {
                    'very_high': len(categories.get('very_high', [])),
                    'high': len(categories.get('high', [])),
                    'moderate': len(categories.get('moderate', [])),
                    'low': len(categories.get('low', [])),
                    'very_low': len(categories.get('very_low', []))
                }
            
            corr_before = count_correlations(correlation_categories_full)
            corr_after = count_correlations(correlation_categories_clean)
            
            print("Korelasyon Seviyesi DeÄŸiÅŸimleri:")
            for level in ['very_high', 'high', 'moderate', 'low', 'very_low']:
                before = corr_before[level]
                after = corr_after[level]
                change = after - before
                change_str = f"{change:+d}" if change != 0 else "0"
                print(f"   {level.replace('_', ' ').title()}: {before} â†’ {after} ({change_str})")
            
            # Veri setlerini global deÄŸiÅŸken olarak sakla
            globals()['uber_df'] = df  # Orijinal veri
            globals()['uber_df_clean'] = best_cleaned_df  # TemizlenmiÅŸ veri
            globals()['dataset_files'] = csv_files
            
            # Tam veri sonuÃ§larÄ±
            globals()['reliability_results_full'] = reliability_results_full
            globals()['reliability_report_full'] = final_report_full
            globals()['correlation_categories_full'] = correlation_categories_full
            
            # TemizlenmiÅŸ veri sonuÃ§larÄ±
            globals()['reliability_results_clean'] = reliability_results_clean
            globals()['reliability_report_clean'] = final_report_clean
            globals()['correlation_categories_clean'] = correlation_categories_clean
            
            # Temizleme bilgileri
            globals()['cleaning_results'] = cleaning_results
            globals()['best_cleaning_method'] = best_method
            globals()['comparison_results'] = comparison_results
            
            print(f"\n{'='*80}")
            print("âœ… KAPSAMLI 2-AÅAMALI ANALÄ°Z TAMAMLANDI!")
            print("="*80)
            print(f"\nğŸ“Š VERÄ° SETLERÄ°:")
            print(f"   ğŸ—‚ï¸  Orijinal: 'uber_df' ({len(df):,} satÄ±r)")
            print(f"   ğŸ§¹ TemizlenmiÅŸ: 'uber_df_clean' ({len(best_cleaned_df):,} satÄ±r)")
            print(f"   ğŸ“ DiÄŸer dosyalar: {[f.name for f in csv_files]}")
            
            print(f"\nğŸ“ˆ GÃœVENÄ°RLÄ°K SKORLARI:")
            print(f"   ğŸ”´ Tam Veri: {final_report_full['final_score']:.1f}/100")
            print(f"   ğŸŸ¢ TemizlenmiÅŸ: {final_report_clean['final_score']:.1f}/100")
            print(f"   ğŸ“Š Ä°yileÅŸme: {improvement:+.1f} puan")
            
            print(f"\nğŸ§¹ TEMÄ°ZLEME BÄ°LGÄ°LERÄ°:")
            print(f"   ğŸ† En iyi yÃ¶ntem: {best_method}")
            print(f"   ğŸ“‰ Veri kaybÄ±: {cleaning_results[best_method]['data_loss_pct']:.2f}%")
            
            print(f"\nğŸ’¡ SONUÃ‡: ")
            if improvement > 5:
                print("   ğŸŒŸ Outlier temizleme Ã¶nemli iyileÅŸme saÄŸladÄ±!")
            elif improvement > 0:
                print("   âœ… Outlier temizleme olumlu etki gÃ¶sterdi.")
            else:
                print("   âš ï¸  Outlier temizleme dikkatli deÄŸerlendirilmeli.")
            
            print(f"\nğŸ¯ Her iki veri seti de analizleriniz iÃ§in hazÄ±r!")
        
        else:
            print("âŒ Veri yÃ¼kleme baÅŸarÄ±sÄ±z!")
    else:
        print("âŒ Veri seti indirme baÅŸarÄ±sÄ±z!")

if __name__ == "__main__":
    main()
